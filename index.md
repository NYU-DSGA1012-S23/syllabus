---
title: Basic Information
layout: default
nav_order: 1
---

# Natural Language Understanding and Computational Semantics

**DS-GA 1012/LING-GA 1012, Spring 2023**<br />
**New York University**

This is an advanced master's level course in natural language processing, with focus on the representation of 
meaning at various linguistic levels. Students will learn basic techniques in deep learning, transfer learning, and 
in-context adaptation while being introduced to natural language understanding tasks such as text classification, 
syntatic and semantic parsing, and question answering. The course will culminate in an original research project 
completed by the student in collaboration with two to three classmates.

## Course Staff

#### Instructor
* Sophie Hao (NYU email: `sophie.hao`)

#### Section Leaders
* Lorena Piedras (NYU email: `lp2535`)
* Namrata Mukhija (NYU email: `nm3571`)

#### Grader
* Artie Shen 


## Logistics

All class sessions take place **in person** in **Room G08** of 
[12 Waverly Place](https://goo.gl/maps/3qye7472KPRqERbi8).

#### Lectures
Tuesdays, 10:00–11:40, with Sophie

#### Lab
Thursdays, 11:15–12:05, with Lorena or Namrata

## Prerequisites

The recommended prerequisite for this course is _Natural Language Processing with Representation Learning_ (DS-GA
1011). You will still be able to register for the course if you have not taken DS-GA 1011, but please be aware that
this is an advanced course. Students are expected to have seen most of the following concepts before.

#### Calculus and Linear Algebra
Partial derivatives, gradients, vectors, matrices, matrix multiplication, vector spaces

#### Probability and Statistics
Probability distributions, conditional probabilities, Bayes's theorem, linear regression

#### Machine Learning and Data Science
Features (discrete vs. continuous), optimization, train/dev/test, dimensionality reduction (e.g., PCA)

#### Python Programming
Basic syntax, Jupyter notebooks, package managers (e.g., `pip`), modules, object-oriented programming

#### Natural Language Processing
Tokenization, vector semantics, language modeling

Since this is a graduate-level course with students from a diverse array of backgrounds (data science, computer 
science, linguistics, and undergrads), many students will be unfamiliar with one or more of the above topics. This 
is okay, as long as you feel comfortable looking up anything that you don't understand or asking for help when 
necessary. 