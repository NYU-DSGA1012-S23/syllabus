---
title: Week 2, Jan. 31/Feb. 2
---

### Basic Techniques 1: Deep Learning

We will learn how to optimize an arbitrary machine learning objective using the stochastic gradient descent 
algorithm and its more popular variant, Adam. We will also learn how automatic differentiation is implemented in the 
PyTorch software library.

Topics
: Stochastic gradient descent, Adam, automatic differentiation, PyTorch

Lecture
: [Slides](https://drive.google.com/file/d/1imOT2z5RObTX8MgU7fwwGvHrrZcmUTY5/view?usp=share_link),
[Zoom Recording](https://nyu.zoom.us/rec/share/NPbiqbsAKOZlfe-b0buy0exLSuIpPDJq9nn7ASWE6ZKnhPFERzKRDjUyN4jGX2MG.j1XxekNe_aa9WNId)

Lab
: [Colab Notebook](https://colab.research.google.com/drive/11b1gFUk5RDilOrMu6aE9da5e71ZeXTo7?usp=sharing),
[Zoom Recording](https://nyu.zoom.us/rec/share/K4rM-AZP0XpXV4ejO5wHaxafvI3jZcMHi0w6UgI2W3gbSI56pKPT-SlbYsIZ4_o.gCN_kVvPA-mPhPcW)

Reading
: **SLP**{: .label .label-yellow } [Chapter 5](https://web.stanford.edu/~jurafsky/slp3/5.pdf), on logistic regression
: **D2L**{: .label .label-yellow } 
[Sections 15.3â€“15.4](https://d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html), on 
training
word2vec
: **Notes**{: .label .label-yellow }
[on stochastic gradient descent and Adam](https://drive.google.com/file/d/1QWyIneavtxtIOvSwKuN4he0AgCCtn5Wm/view?usp=share_link)
: **Notes**{: .label .label-yellow }
[on backpropagation in PyTorch](https://drive.google.com/file/d/1rKR8Kcj61SY5rifJIo9OaERPvEtXjelv/view?usp=share_link)
: [Olah (2015a)](https://colah.github.io/posts/2015-08-Backprop/), Calculus on Computational Graphs: Backpropagation
(blog post)

Deadlines
: **HW 0 Due**{: .label .label-red }