---
title: Week 2, Jan. 31/Feb. 2
---

### Basic Techniques 1: Deep Learning

We will learn how to optimize an arbitrary machine learning objective using the stochastic gradient descent algorithm.
We then learn how word2vec embeddings are trained while introducing the conceptual framework of _neural networks_.

Topics
: Stochastic gradient descent, Adam, automatic differentiation

Lab
: Introduction to supervised learning

Reading
: **SLP**{: .label .label-yellow } [Chapter 5](https://web.stanford.edu/~jurafsky/slp3/5.pdf), on logistic regression
: **D2L**{: .label .label-yellow } 
[Sections 15.3â€“15.4](https://d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html), on 
training
word2vec
: **Notes**{: .label .label-yellow }
[on stochastic gradient descent and Adam](https://drive.google.com/file/d/1QWyIneavtxtIOvSwKuN4he0AgCCtn5Wm/view?usp=share_link)
: **Notes**{: .label .label-yellow }
[on backpropagation in PyTorch](https://drive.google.com/file/d/1rKR8Kcj61SY5rifJIo9OaERPvEtXjelv/view?usp=share_link)
: [Olah (2015a)](https://colah.github.io/posts/2015-08-Backprop/), Calculus on Computational Graphs: Backpropagation
(blog post)

Deadlines
: **HW 0 Due**{: .label .label-red }