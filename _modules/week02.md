---
title: Week 2, Jan. 31/Feb. 2
---

### Basic Techniques 1: Deep Learning

We will learn how to optimize an arbitrary machine learning objective using the backpropagation and stochastic gradient
descent algorithms. We then learn how word2vec embeddings are trained.

Topics
: Stochastic gradient descent, Adam, backpropagation

Reading
: [Hao (2023)](https://drive.google.com/file/d/1QWyIneavtxtIOvSwKuN4he0AgCCtn5Wm/view?usp=share_link), Sophie's notes on
stochastic gradient descent
: [Olah (2015a)](https://colah.github.io/posts/2015-08-Backprop/), Calculus on Computational Graphs: Backpropagation
(blog post)

Deadlines
: **HW 0 Due**{: .label .label-red }