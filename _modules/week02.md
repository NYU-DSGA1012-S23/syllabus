---
title: Week 2, Jan. 31/Feb. 2
---

### Basic Techniques 1: Deep Learning

We will learn how to optimize an arbitrary machine learning objective using the stochastic gradient descent algorithm.
We then learn how word2vec embeddings are trained while introducing the conceptual framework of _neural networks_.

Topics
: Stochastic gradient descent, Adam, automatic differentiation

Reading
: * **D2L**{: .label .label-yellow } [chapter 1](https://d2l.ai/chapter_introduction/index.html);
sections [2.1–2.2](https://d2l.ai/chapter_preliminaries/ndarray.html),
[2.5](https://d2l.ai/chapter_preliminaries/autograd.html),
[15.3–15.4](https://d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)
: * **Notes**{: .label .label-yellow }
[on stochastic gradient descent and Adam](https://drive.google.com/file/d/1QWyIneavtxtIOvSwKuN4he0AgCCtn5Wm/view?usp=share_link)
: [Olah (2015a)](https://colah.github.io/posts/2015-08-Backprop/), Calculus on Computational Graphs: Backpropagation
(blog post)

Deadlines
: **HW 0 Due**{: .label .label-red }