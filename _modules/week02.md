---
title: Week 2, Jan. 31/Feb. 2
---

### Basic Techniques 1: Deep Learning

We will learn how to optimize an arbitrary machine learning objective using the stochastic gradient descent algorithm.
We then learn how word2vec embeddings are trained while introducing the conceptual framework of _neural networks_.

Topics
: Stochastic gradient descent, Adam, automatic differentiation

Reading
: [Zhang et al. (2021)](https://d2l.ai/) chapter [1 (introduction)](https://d2l.ai/chapter_introduction/index.html); 
sections [2.1–2.2 (working with data)](https://d2l.ai/chapter_preliminaries/ndarray.html), 
[2.5 (automatic differentiation)](https://d2l.ai/chapter_preliminaries/autograd.html), 
[15.3–15.4 (training word2vec)](https://d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)
: [Hao (2023)](https://drive.google.com/file/d/1QWyIneavtxtIOvSwKuN4he0AgCCtn5Wm/view?usp=share_link), Sophie's notes on
stochastic gradient descent
: [Olah (2015a)](https://colah.github.io/posts/2015-08-Backprop/), Calculus on Computational Graphs: Backpropagation
(blog post)

Deadlines
: **HW 0 Due**{: .label .label-red }