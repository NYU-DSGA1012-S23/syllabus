---
title: Week 12, Apr. 18/20
---

### General Knowledge and General Intelligence

Large language models like GPT-3 are now being used as databases of general knowledge and serving as general-purpose 
language assistants. This week we study the problem of _alignment_: how to control the behavior of a general-purpose 
language model so that it adheres to constraints not entailed by its pre-training objective.

Topics
: Alignment, large language models, imitation learning, preference modeling, RLHF, red-teaming, debate, 
constitutional AI

Lecture
: [Slides](https://drive.google.com/file/d/1d_hGe3gI-GZJGk-camDAUaMrDwmAfd6R/view?usp=share_link), [Zoom Recording](https://nyu.zoom.us/rec/share/nbSASB6bs5GXQcXS_asdd7qfS1am3AgiMFcckLU3lsRDoMzh27YYSVZ0oBZ1_RJf.-mC9_Zjzw1ZoXSIP)

Lab
: No lab this week!

Readings
: [Lin et al. (2022)](https://aclanthology.org/2022.acl-long.229/), the TruthfulQA benchmark, which tests for 
imitative falsehoods
: [Askell et al. (2021)](https://arxiv.org/abs/2112.00861), on the HHH criterion
: [Perez et al. (2022)](https://arxiv.org/abs/2202.03286), on red-teaming
: [Irving et al. (2018)](https://arxiv.org/abs/1805.00899), on debate ([blog post version](https://openai.com/research/debate))
: [Vox article](https://www.vox.com/future-perfect/2022/8/8/23150496/effective-altruism-sam-bankman-fried-dustin-moskovitz-billionaire-philanthropy-crytocurrency) on Effective altruism
