---
title: Week 12, Apr. 18/20
---

### General Knowledge and General Intelligence

Large language models like GPT-3 are now being used as databases of general knowledge and serving as general-purpose 
language assistants. This week we study the problem of _alignment_: how to control the behavior of a general-purpose 
language model so that it adheres to constraints not entailed by its pre-training objective.

Topics
: Alignment, large language models, imitation learning, preference modeling, RLHF, red-teaming, debate, 
constitutional AI, longtermism

Lecture
: [Slides](https://drive.google.com/file/d/1d_hGe3gI-GZJGk-camDAUaMrDwmAfd6R/view?usp=share_link), [Zoom Recording](https://nyu.zoom.us/rec/share/nbSASB6bs5GXQcXS_asdd7qfS1am3AgiMFcckLU3lsRDoMzh27YYSVZ0oBZ1_RJf.-mC9_Zjzw1ZoXSIP)

Lab
: No lab this week!

Readings
: [Lin et al. (2022)](https://aclanthology.org/2022.acl-long.229/), the TruthfulQA benchmark, which tests for 
imitative falsehoods
: [Askell et al. (2021)](https://arxiv.org/abs/2112.00861), on the HHH criterion
: [Perez et al. (2022)](https://arxiv.org/abs/2202.03286), on red-teaming
: [Irving et al. (2018)](https://arxiv.org/abs/1805.00899), on debate ([blog post version](https://openai.com/research/debate))
: [New Yorker article](https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism) 
that narrates the origins of Effective Altruism
: Opinion pieces [for](https://www.effectivealtruism.org/articles/longtermism) and [against](https://www.vox.com/future-perfect/23298870/effective-altruism-longtermism-will-macaskill-future) longtermism
: [Sam Bowman's slides](https://drive.google.com/file/d/1m0l9J9e4kDDtvXWSnQZGplEncGRwkpkc/edit) from last year's 
version of this course
: [Slides](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec22.pdf) from Princeton's course 
on large language models
: The [NYU Alignment Research Group](https://wp.nyu.edu/arg/people/)
