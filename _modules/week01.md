---
title: Week 1, Jan. 24/26
---

### What Is Meaning?

We will introduce the concept of meaning in natural language, taking inspiration from linguists, philosophers, and data
scientists. We will learn about the word2vec model of semantics and examine in what sense and to what extent it models
"meaning."

Topics
: Lexical semantics, word embeddings, tokenization

Reading
: [Bender (2013)](https://www.morganclaypool.com/doi/abs/10.2200/S00493ED1V01Y201303HLT020) \#7–16, on the concept of a “word”
: [Bender and Lascarides (2019)](https://www.morganclaypool.com/doi/abs/10.2200/S00935ED1V02Y201907HLT043) \#18, \#19,
\#21–24, on lexical semantics
: [Hao (2023)](https://drive.google.com/file/d/16vWNLaCFEmnW2kxhsCGkd5OXPEMgKl7v/view?usp=share_link), Sophie's notes on
vector semantics (skip-gram with negative sampling)
: [Zhang et al. (2021)](https://d2l.ai/) sections 
[15.1 (skip-gram and CBOW)](https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html), 
[15.5 (GloVe)](https://d2l.ai/chapter_natural-language-processing-pretraining/glove.html), 
[15.6 (subword embeddings)](https://d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html), 
[15.7 (analogies)](https://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)
: [Bolukbasi et al. (2016)](https://arxiv.org/abs/1607.06520), on manipulating word embeddings