---
title: Week 4, Feb. 14/16
---

### Basic Techniques 3: Sequence Modeling

We introduce the simple recurrent network, long short-term memory network, and the Transformer, which are neural 
network architectures designed to learn embeddings for sequences of word embeddings. We also introduce the task of 
natural language inference, and learn about the concepts of entailment, presupposition, and implicature.

Topics
: RNNs, LSTMs, Transformers, natural language inference, pragmatics (entailment, presupposition, implicature, 
Grice's maxims)

Lecture
: [Slides](https://drive.google.com/file/d/1hxwbBAZDzuhPDhm_0-mb-xuPEeLovsM5/view?usp=sharing),
[Zoom Recording](https://nyu.zoom.us/rec/share/YvBGc1Li1EohCok_ZwHI9vvUae4soS3TliGH-x75JmPGHwTojyYLUHJwIzbvmcHq.S-rv0uYasJK6-osW )


Lab
: [Colab Notebook](https://colab.research.google.com/drive/19u3bxWWeoOGD2eb5AtjSQUDiYbgdTDDl?usp=sharing)
(No Zoom recording due to technical issue)

Reading
: [Olah (2015b)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), Understanding LSTM Networks (blog post)
: [Alammar (2018)](https://jalammar.github.io/illustrated-transformer/), The Illustrated Transformer (blog post)
: **D2L**{: .label .label-yellow }
[Sections 16.1–16.2](https://d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html),
on sentiment analysis using RNNs
: **Ling2**{: .label .label-yellow }
[\#77–78](https://www.morganclaypool.com/doi/abs/10.2200/S00935ED1V02Y201907HLT043), on entailment and presupposition
: [Bowman et al. (2015)](https://aclanthology.org/D15-1075/), the Stanford Natural Language Inference
corpus [(website)](https://nlp.stanford.edu/projects/snli/)
