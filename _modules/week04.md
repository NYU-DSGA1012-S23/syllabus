---
title: Week 4, Feb. 14/16
---

### Basic Techniques 2: Transfer Learning

We introduce _transfer learning_, a technique where large quantities of unlabeled data can be leveraged by 
pre-training an encoder network on a language modeling objective. We survey common pre-trained models such as BERT 
and GPT-2.

Topics
: Pre-trained models, BERT, contextualized embeddings

Reading
: **SLP**{: .label .label-yellow } [Chapter 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf), on Transformer 
language models
: **D2L**{: .label .label-yellow }
[Sections 15.8â€“15.10](https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html), on BERT
: [Devlin et al. (2019)](https://aclanthology.org/N19-1423/), the original BERT paper