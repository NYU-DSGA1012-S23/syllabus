---
title: Week 4, Feb. 14/16
---

### Basic Techniques 2: Transfer Learning

We introduce _transfer learning_, a technique where large quantities of unlabeled data can be leveraged by 
pre-training an encoder network on a language modeling objective. We survey common pre-trained models such as BERT 
and GPT-2.

Topics
: Pre-trained models, BERT, contextualized embeddings

Reading
: **SLP**{: .label .label-yellow } [Chapters 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf) and
[11](https://web.stanford.edu/~jurafsky/slp3/11.pdf), on pre-training
: **D2L**{: .label .label-yellow }
[Sections 15.8â€“15.10](https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html), on BERT
: [Ruder (2019)](https://ruder.io/state-of-transfer-learning-in-nlp/), The State of Transfer Learning in NLP (blog post)
: [Devlin et al. (2019)](https://aclanthology.org/N19-1423/), the original BERT paper