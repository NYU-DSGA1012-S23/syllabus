---
title: Week 4, Feb. 14/16
---

### Basic Techniques 2: Transfer Learning

We introduce _transfer learning_, a technique where large quantities of unlabeled data can be leveraged by 
pre-training an encoder network on a language modeling objective. We survey common pre-trained models such as BERT 
and GPT-2.

Topics
: Pre-trained models, BERT, contextualized embeddings

Lecture
: [Slides](https://drive.google.com/file/d/1hxwbBAZDzuhPDhm_0-mb-xuPEeLovsM5/view?usp=sharing),
[Zoom Recording](https://nyu.zoom.us/rec/share/YvBGc1Li1EohCok_ZwHI9vvUae4soS3TliGH-x75JmPGHwTojyYLUHJwIzbvmcHq.S-rv0uYasJK6-osW )


Lab
: [Colab Notebook](https://colab.research.google.com/drive/19u3bxWWeoOGD2eb5AtjSQUDiYbgdTDDl?usp=sharing)

Reading
: **SLP**{: .label .label-yellow } [Chapter 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf), on Transformer 
language models
: **D2L**{: .label .label-yellow }
[Sections 15.8â€“15.10](https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html), on BERT
: [Devlin et al. (2019)](https://aclanthology.org/N19-1423/), the original BERT paper