---
title: Week 6, Feb. 28/Mar. 2
---

### Basic Techniques 3: In-Context Adaptation

We take the idea of transfer learning a step further with _in-context adaptation_. In this approach, tasks are performed
by large, general-purpose language models by having them auto-complete a prompt that describes the task and the input.
There is no fine-tuning or other explicit training on the target task.

Reading
: [Radford et al. (2019)](https://openai.com/blog/better-language-models/), Better Language Models and Their
Implications (blog post)
: [Brown et al. (2020)](https://arxiv.org/abs/2005.14165), the GPT-3 model (Read sections 1, 2, and 4, and pick two 
subsections of section 3 to read)


Deadlines
: **Project Mini-Proposal Due**{: .label .label-blue }