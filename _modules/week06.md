---
title: Week 6, Feb. 28/Mar. 2
---

### Basic Techniques 3: In-Context Adaptation

We take the idea of transfer learning a step further with _in-context adaptation_. In this approach, tasks are performed
by large, general-purpose language models by having them auto-complete a prompt that describes the task and the input.
There is no fine-tuning or other explicit training on the target task.

Lecture
: [Slides](https://drive.google.com/file/d/1KNJvUnZGiSrCEFAw4bPOaDTziv6jf2_K/view?usp=share_link),
[Zoom Recording](https://nyu.zoom.us/rec/share/m5Vd5HVifEhOSjjpvpL15OScctCxq9pd9kmbluUfl-FW-VN9c6OXYqA2IPJC2hYU.NIj0FGE-gt75xKHt)

Lab
: [Slides](https://drive.google.com/file/d/1tbAms33Kiz7zH81lvkc9ugeVaUXzu2e0/view?usp=share_link),
[Zoom Recording](https://nyu.zoom.us/rec/share/xK1BzYJEEHwBqxqAF8LjzFS6jgZiQABaN3VUr_egcUXmD_OBBYPQEe4Yudqr6Uap.JzT8ho940HeDYJki)

Reading
: [Radford et al. (2019)](https://openai.com/blog/better-language-models/), Better Language Models and Their
Implications (blog post)
: [Brown et al. (2020)](https://arxiv.org/abs/2005.14165), the GPT-3 model (Read sections 1, 2, and 4, and pick two 
subsections of section 3 to read)


Deadlines
: **HW 2 Due 2/27**{: .label .label-red }
: **EC 2 Due 2/27**{: .label .label-green }
: **Project Mini-Proposal Due**{: .label .label-blue }