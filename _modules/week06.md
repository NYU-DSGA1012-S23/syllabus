---
title: Week 6, Feb. 28/Mar. 2
---

### Basic Techniques 3: In-Context Adaptation

We take the idea of transfer learning a step further with _in-context adaptation_. In this approach, tasks are performed
by large, general-purpose language models by having them auto-complete a prompt that describes the task and the input.
There is no fine-tuning or other explicit training on the target task.

Reading
: [Radford et al. (2019)](https://openai.com/blog/better-language-models/), Better Language Models and Their
Implications (blog post)
: [Brown et al. (2020)](https://arxiv.org/abs/2005.14165), the original GPT-3 paper (Warning: It's very long! You 
don't have to read all of it.)
: [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), an example of sophisticated prompting 

Deadlines
: **Project Mini-Proposal Due**{: .label .label-blue }