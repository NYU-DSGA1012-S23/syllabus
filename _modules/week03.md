---
title: Week 3, Feb. 7/9
---

### Entailment, Presupposition, and Reasoning

In order to "understand" meaning and language, a model needs to know what statements are _entailed_ by others. Using the
task of _natural language inference_ as a running example, we will introduce common neural network architectures in NLU
such as the multi-layer perceptron, the recurrent neural network, and the Transformer.

Topics
: Entailment, presupposition, text classification, natural language inference, neural architectures (MLPs, RNNs, LSTMs,
attention mechanisms, Transformers)

Reading
: [Bender and Lascarides (2019)](https://www.morganclaypool.com/doi/abs/10.2200/S00935ED1V02Y201907HLT043) #77â€“78, on
entailment and presupposition
: [Bowman et al. (2015)](https://aclanthology.org/D15-1075/), the Stanford Natural Language Inference
corpus [(website)](https://nlp.stanford.edu/projects/snli/)
: [Olah (2015b)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), Understanding LSTM Networks (blog post)
: [Alammar (2018)](https://jalammar.github.io/illustrated-transformer/), The Illustrated Transformer (blog post)

Deadlines
: **HW 1 Due**{: .label .label-red }
: **EC 1 Due**{: .label .label-green }