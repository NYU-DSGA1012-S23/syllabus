---
title: Week 3, Feb. 7/9
---

### Entailment, Presupposition, and Reasoning

We introduce two tasks that require models to "understand" language to a certain extent: _sentiment analysis_ and
_natural language inference_. Using these tasks as running examples, we will introduce common neural network
architectures in NLU such as the multi-layer perceptron, the recurrent neural network, and the Transformer.

Topics
: Entailment, presupposition, text classification, natural language inference, neural architectures (MLPs, RNNs, LSTMs,
attention mechanisms, Transformers)

Lab
: Text classification in PyTorch

Reading
: **SLP**{: .label .label-yellow } [Chapter 25](https://web.stanford.edu/~jurafsky/slp3/25.pdf), on sentiment analysis
: **D2L**{: .label .label-yellow }
[Sections 16.1–16.2](https://d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html), 
on sentiment analysis using RNNs
: **Ling2**{: .label .label-yellow }
[\#77–78](https://www.morganclaypool.com/doi/abs/10.2200/S00935ED1V02Y201907HLT043), on entailment and presupposition
: [Bowman et al. (2015)](https://aclanthology.org/D15-1075/), the Stanford Natural Language Inference
corpus [(website)](https://nlp.stanford.edu/projects/snli/)
: [Olah (2015b)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), Understanding LSTM Networks (blog post)
: [Alammar (2018)](https://jalammar.github.io/illustrated-transformer/), The Illustrated Transformer (blog post)

Deadlines
: **HW 1 Due**{: .label .label-red }
: **EC 1 Due**{: .label .label-green }