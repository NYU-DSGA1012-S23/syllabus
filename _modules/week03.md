---
title: Week 3, Feb. 7/9
---

### Basic Techniques 1: Deep Learning

We will learn how to define, train, and test a deep neural network model using the backpropagation and stochastic
gradient descent algorithms. We will introduce common neural network architectures in NLU such as the multi-layer
perceptron, the recurrent neural network, and the Transformer.

Topics
: Neural architectures (MLP, RNN, LSTM, Transformer), automatic differentiation, the PyTorch software package, 
hyperparameter tuning

Reading
: [Hao (2021)](https://drive.google.com/file/d/1bacO_yrfX940LQsfSCAhuavcHpd_JqWB/view?usp=share_link), Sophie's
notes on gradient descent
: [Hao (2023)](https://drive.google.com/file/d/1rKR8Kcj61SY5rifJIo9OaERPvEtXjelv/view?usp=share_link), Sophie's 
notes on backpropagation in PyTorch
: [Olah (2015a)](https://colah.github.io/posts/2015-08-Backprop/), Calculus on Computational Graphs: Backpropagation 
(blog post)
: [Olah (2015b)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), Understanding LSTM Networks (blog post)
: [Alammar (2018)](https://jalammar.github.io/illustrated-transformer/), The Illustrated Transformer (blog post)

Deadlines
: **HW 1 Due**{: .label .label-red }
: **EC 1 Due**{: .label .label-green }