---
title: Week 5, Feb. 21/23
---

### Basic Techniques 4: Transfer Learning

We introduce _transfer learning_, a technique where large quantities of unlabeled data can be leveraged by
pre-training an encoder network on a language modeling objective. A guest lecturer, NYU graduate 
student [Jason Phang](https://jasonphang.com/), will tell us how modern engineering techniques can allow us to do 
fine-tuning at scale.

Topics
: The BERT model, pre-training, fine-tuning, parallel computation, GPUs

Lecture
: [Sophie's Slides](https://drive.google.com/file/d/1Qz8ozH370f1OcqSisUnw79SfFO26KUG5/view?usp=share_link),
[Jason's Slides](https://drive.google.com/file/d/1N7F4_19B5v3Nld-ePu-wnwlduUKh0jcn/view?usp=share_link),
[Zoom Recording](https://nyu.zoom.us/rec/share/IGlQsDw0pzxywuZTi2Mnr7JiX1qFfrN1A8f6_Qiw6szWHhNNTELkAg4Dsl20dbXU.y_FwOI5BP2qQKh3k)

Lab
: [Slides](https://drive.google.com/file/d/1JpYGuTausigXmi5yyCEXOn9l2ykOJwcu/view?usp=share_link),
[Zoom Recording](https://nyu.zoom.us/rec/share/4c9mJOjDlCrY_U2rXjyru7P0LJ-pVZ1mDZP9J-yPUu_RVpe5sEd9HPBTDC83AFRv.D9UmhXu5ZwL-VXbr)

Readings
: **SLP**{: .label .label-yellow } [Chapter 10](https://web.stanford.edu/~jurafsky/slp3/10.pdf), on Transformer
language models
: **SLP**{: .label .label-yellow } [Chapter 11](https://web.stanford.edu/~jurafsky/slp3/11.pdf), on fine-tuning
: **D2L**{: .label .label-yellow }
[Sections 15.8–15.10](https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html), on BERT
: **D2L**{: .label .label-yellow }
[Sections 16.6–16.7](https://d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html), on 
fine-tuning BERT
: [Devlin et al. (2019)](https://aclanthology.org/N19-1423/), the original BERT paper
: [Ruder (2019)](https://ruder.io/state-of-transfer-learning-in-nlp/), The State of Transfer Learning in NLP (blog post)

